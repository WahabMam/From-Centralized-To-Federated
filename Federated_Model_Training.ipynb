{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Model Training\n",
    "\n",
    "In the previous notebook, **Centralized_Model_Training**, we explored the limitations of centralized training, where models can struggle to generalize when certain classes or patterns are excluded from the training data. We observed that models trained under these conditions might produce incorrect predictions, highlighting the need for a more robust approach to training.\n",
    "\n",
    "### In the 'Centralized_Training' notebook, we have seen that three models are trained on three different datasets. When we tested the models on data that they had not seen during training, the models did not perform well, and some even had an accuracy of zero. \n",
    "\n",
    "Data volume and diversity are critical for training good models, but the data is often distributed. Traditional training approaches, like the one used in **Centralized_Training**, assume centralized data, making it difficult or even impossible to centralize the data due to privacy concerns, regulations, and the sheer volume of data.\n",
    "\n",
    "### Federated Learning (FL) is the solution to this problem. It operates on distributed data, allowing models to be trained across various devices and organizations while keeping the data localized. FL can be applied across different industries and organizational silos, providing a way to enhance model training without compromising data privacy.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "1. Understand the fundamentals of federated learning and its advantages over traditional centralized training.\n",
    "2. Implement federated learning using the Flower framework.\n",
    "3. Train models on data distributed across different clients while maintaining data privacy.\n",
    "4. Evaluate the performance of the federated model and compare it with the centralized approach.\n",
    "\n",
    "Let's begin by setting up the necessary libraries and configurations for our federated learning experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.common import ndarrays_to_parameters, Context\n",
    "from flwr.server import ServerApp, ServerConfig\n",
    "from flwr.server import ServerAppComponents\n",
    "from flwr.server.strategy import FedAvg\n",
    "from flwr.simulation import run_simulation\n",
    "\n",
    "from Utils2 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and Preparing the MNIST Dataset\n",
    "\n",
    "After importing the necessary libraries and the `Utils2.py` file, we will download the MNIST dataset. Then, we will split the dataset into three parts and exclude specific digits from each part to create distinct training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.MNIST(\n",
    "    \"./MNIST_data/\", download=True, train=True, transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "# Calculate the total length and split the dataset into three parts\n",
    "total_length = len(trainset)\n",
    "split_size = total_length // 3\n",
    "torch.manual_seed(42)\n",
    "part1, part2, part3 = random_split(trainset, [split_size] * 3)\n",
    "\n",
    "\n",
    "# Exclude specific digits from each part\n",
    "part1 = exclude_digits(part1, excluded_digits=[1, 3, 7])\n",
    "part2 = exclude_digits(part2, excluded_digits=[2, 5, 8])\n",
    "part3 = exclude_digits(part3, excluded_digits=[4, 6, 9])\n",
    "\n",
    "# Store the training sets in a list\n",
    "train_sets = [part1, part2, part3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Test Dataset\n",
    "\n",
    "Next, we will download the MNIST test dataset. This dataset will be used to evaluate the performance of our federated learning model. Additionally, we will include the digits that were excluded from the training sets to assess the model's ability to generalize to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in `testset`: 10000\n"
     ]
    }
   ],
   "source": [
    "# Download the MNIST test dataset\n",
    "testset = datasets.MNIST(\n",
    "    \"./MNIST_data/\", download=True, train=False, transform=transform\n",
    ")\n",
    "print(\"Number of examples in `testset`:\", len(testset))\n",
    "\n",
    "# Include specific digits that were excluded during training\n",
    "testset_137 = include_digits(testset, [1, 3, 7])\n",
    "testset_258 = include_digits(testset, [2, 5, 8])\n",
    "testset_469 = include_digits(testset, [4, 6, 9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Parameter Exchange in Federated Learning\n",
    "\n",
    "In federated learning, exchanging model parameters between the server and clients is essential. When a client receives model parameters from the server, it updates its local model with those new parameters. After completing the training, the client sends the updated model parameters back to the server. To facilitate this exchange, we need two functions: `set_weights()` and `get_weights()`. These functions are used for the client-server exchange of training information.\n",
    "The functions are defined as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the parameters of the model\n",
    "def set_weights(net, parameters):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict(\n",
    "        {k: torch.tensor(v) for k, v in params_dict}\n",
    "    )\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "# Retrieves the parameters from the model\n",
    "def get_weights(net):\n",
    "    ndarrays = [\n",
    "        val.cpu().numpy() for _, val in net.state_dict().items()\n",
    "    ]\n",
    "    return ndarrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FlowerClient Class in Federated Learning\n",
    "\n",
    "To connect our training and evaluation pipeline in federated learning, we define a `FlowerClient` class, which is a subclass of the `NumPyClient` class. The `FlowerClient` class typically includes two key methods: the `fit` method and the `evaluate` method.\n",
    "\n",
    "- **`fit` Method**: This method is responsible for training the neural network using the provided parameters and the local training dataset.\n",
    "- **`evaluate` Method**: This method evaluates the performance of the neural network using the provided parameters and the local test dataset.\n",
    "\n",
    "The `FlowerClient` class is defined as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerClient(NumPyClient):\n",
    "    def __init__(self, net, trainset, testset):\n",
    "        self.net = net\n",
    "        self.trainset = trainset\n",
    "        self.testset = testset\n",
    "\n",
    "    # Train the model\n",
    "    def fit(self, parameters, config):\n",
    "        set_weights(self.net, parameters)  # Set the model weights\n",
    "        train_model(self.net, self.trainset)  # Train the model on local dataset\n",
    "        return get_weights(self.net), len(self.trainset), {}   # Return updated weights, number of examples, and additional information\n",
    "\n",
    "    # Test the model\n",
    "    def evaluate(self, parameters: NDArrays, config: Dict[str, Scalar]):\n",
    "        set_weights(self.net, parameters)  # Set the model weights\n",
    "        loss, accuracy = evaluate_model(self.net, self.testset)    # Evaluate the model on the local test dataset\n",
    "        return loss, len(self.testset), {\"accuracy\": accuracy}     # Return loss, number of examples, and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Client Function for Federated Learning\n",
    "\n",
    "To enable the Flower framework to create client instances as needed, we utilize a function called `client_fn`. This function is responsible for creating Flower client instances on demand, which is essential for resource utilization and optimization.\n",
    "\n",
    "By using this function, federated training can easily span hundreds of clients and can be efficiently simulated on a single machine. The Flower framework calls the `client_fn` function whenever it requires an instance of a specific client to invoke the `fit` or `evaluate` methods of the `FlowerClient` class.\n",
    "\n",
    "The `client_fn` function is defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client function\n",
    "def client_fn(context: Context) -> Client:\n",
    "    net = SimpleModel() # Create an instance of the SimpleModel\n",
    "    partition_id = int(context.node_config[\"partition-id\"])   # Retrieve the partition ID from context\n",
    "    client_train = train_sets[int(partition_id)] # Get the corresponding training set for the client\n",
    "    client_test = testset  # Use the shared test set for evaluation\n",
    "    return FlowerClient(net, client_train, client_test).to_client()   # Return the FlowerClient instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a ClientApp Instance\n",
    "\n",
    "Next, we create an instance of `ClientApp` by passing the previously defined `client_fn`. The `ClientApp` serves as the entry point for everything happening on the client side, facilitating communication between the client and the server in the federated learning setup.\n",
    "\n",
    "The following code snippet demonstrates how to create the `ClientApp` instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of ClientApp by passing the previously defined client_fn\n",
    "client = ClientApp(client_fn=client_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server-Side Evaluation Function\n",
    "\n",
    "Now we need to establish the server-side counterpart that aggregates the models received from the clients and evaluates the performance of the global model. For this purpose, we need to define the following evaluation function called `evaluate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(server_round, parameters, config):\n",
    "    net = SimpleModel()\n",
    "    set_weights(net, parameters)\n",
    "\n",
    "\n",
    "    # assess the accuracy on full MNINST dataseta and alsot on diffrent subsets.\n",
    "    _, accuracy = evaluate_model(net, testset)\n",
    "    _, accuracy137 = evaluate_model(net, testset_137)\n",
    "    _, accuracy258 = evaluate_model(net, testset_258)\n",
    "    _, accuracy469 = evaluate_model(net, testset_469)\n",
    "\n",
    "    log(INFO, \"test accuracy on all digits: %.4f\", accuracy)\n",
    "    log(INFO, \"test accuracy on [1,3,7]: %.4f\", accuracy137)\n",
    "    log(INFO, \"test accuracy on [2,5,8]: %.4f\", accuracy258)\n",
    "    log(INFO, \"test accuracy on [4,6,9]: %.4f\", accuracy469)\n",
    "\n",
    "    if server_round == 3:\n",
    "        cm = compute_confusion_matrix(net, testset)\n",
    "        plot_confusion_matrix(cm, \"Final Global Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server Application and Strategy Definition\n",
    "\n",
    "To create a server application, we first need to determine which strategy we want to implement. The strategy serves as an abstraction that implements the server-side federated learning algorithm. In this case, we will be using the FedAvg strategy (Federated Averaging), which is commonly employed in federated learning scenarios.\n",
    "\n",
    "Hereâ€™s how we define the server function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SimpleModel()\n",
    "params = ndarrays_to_parameters(get_weights(net))  #  initialized a SimpleModel instance and retrieve its initial parameters using the get_weights function.\n",
    "# These parameters will be used as the starting point for the federated learning process.\n",
    "\n",
    "def server_fn(context: Context):\n",
    "    strategy = FedAvg(\n",
    "        fraction_fit=1.0, # Sample 100% of available clients for training\n",
    "        fraction_evaluate=0.0, \n",
    "        initial_parameters=params,\n",
    "        evaluate_fn=evaluate, # the function used for server side evaluation \n",
    "    )\n",
    "    config=ServerConfig(num_rounds=3)\n",
    "    return ServerAppComponents(\n",
    "        strategy=strategy,\n",
    "        config=config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Server Application\n",
    "\n",
    "Now that we have defined our server function and strategy, we can create an instance of `ServerApp`. This app will manage the federated learning process, coordinating the training and evaluation between clients and the server.\n",
    "\n",
    "Here's how we can instantiate the `ServerApp`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the ServerApp\n",
    "server = ServerApp(server_fn=server_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Simulation\n",
    "\n",
    "A real-life federated learning system is distributed, typically consisting of one server and several distributed devices (clients). For simplicity in this example, we will simulate such a system by running everything (both server and clients) on a single machine. \n",
    "\n",
    "To achieve this, we can use the Flower function `run_simulation`, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=3, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Evaluating initial global parameters\n",
      "2024-10-17 20:25:49,206\tWARNING packaging.py:406 -- File c:\\Users\\wahab\\Desktop\\Flower_Demo\\MNIST_data\\MNIST\\raw\\train-images-idx3-ubyte is very large (44.86MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['c:\\Users\\wahab\\Desktop\\Flower_Demo\\MNIST_data\\MNIST\\raw\\train-images-idx3-ubyte']})`\n",
      "\u001b[92mINFO \u001b[0m:      test accuracy on all digits: 0.1267\n",
      "\u001b[92mINFO \u001b[0m:      test accuracy on [1,3,7]: 0.2275\n",
      "\u001b[92mINFO \u001b[0m:      test accuracy on [2,5,8]: 0.1201\n",
      "\u001b[92mINFO \u001b[0m:      test accuracy on [4,6,9]: 0.0380\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=22196)\u001b[0m ++++++++++ client intilized ++++++++\n"
     ]
    }
   ],
   "source": [
    "# Initiate the simulation passing the server and client apps\n",
    "# Specify the number of super nodes that will be selected on every round\n",
    "run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Remarks\n",
    "\n",
    "In the above results, the **[INIT]** phase indicates that the system is using the initial global parameters provided by the strategy. It then evaluates these global model parameters using the evaluation function we defined earlier. This evaluation includes assessing the test accuracy on all digits as well as on the three different subsets we established previously.\n",
    "\n",
    "The process continues through **[ROUND 1]**, **[ROUND 2]**, and **[ROUND 3]** respectively, showcasing how the model evolves over successive training rounds. Each round involves aggregating updates from clients, enhancing the model's performance by leveraging the diversity of data from different clients.\n",
    "\n",
    "This federated learning approach demonstrates the potential for improved model accuracy and robustness, especially when dealing with non-IID (independent and identically distributed) data across multiple sources.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flwr-deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
